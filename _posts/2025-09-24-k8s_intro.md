---
title: 'k8s'
date: 2025-09-24
permalink: /posts/k8s_intro/
tags:
  - infra
---

# k8s and Docker

### What can Docker do? And what can it not do?

We often face a problem where code runs on our machine but not on someone else's. Docker solves this by acting like a **Standardized Container**. You can package applications, libraries, dependencies, and configuration files into a container. This container can then be hosted on different machines and will run with the same results.

However, there are some things Docker alone cannot do. For example, imagine a container running a popular web service. As it gets more traffic, the website needs to be scaled out.

Docker by itself cannot handle:

*   **Scaling:** What if I want to run 50 containers at the same time?
*   **Load Balancing:** When a client request comes in, how do I distribute the traffic across these 50 containers?
*   **Self-healing:** If three containers crash, how does the system find out and replace them?
*   **Service Discovery:** I also have a database running in another Docker container. If it dies, how does the application container find the new IP address of the database?
*   **Zero-downtime Deployment:** If I want to upgrade my application to v2, how can I do it without affecting users?

This is where we need Kubernetes (k8s).

Kubernetes is designed to "orchestrate" many containers. We use YAML files to declare what we need. For example, we can specify that we need an `nginx:latest` image running with port 80 open. If the service dies, Kubernetes will start another container to replace it immediately.

You can think of Kubernetes as a General, working 24/7 to keep your services running. It helps to monitor, make decisions, and load balance.

The process looks like this:
`Us -> kubectl (command-line tool) -> Kubernetes API Server (the commander) -> (K8s) -> Kubelet (worker node agent) -> Docker (on worker node) -> Container Runs`



### Kubernetes API

The `kube-apiserver` is the gateway to the Kubernetes cluster.

Everything, including commands from `kubectl`, communication between components, and checking cluster status, must go through the `kube-apiserver`. It is the front door of the control plane.

Every request that comes to the API server is checked for authentication and authorization, similar to a security guard. After that, it checks the RBAC (Role-Based Access Control) policies to decide if the user has permission to proceed.

Even if you have permission, the API server will also validate your command. For example, if you want to start a new pod, but your YAML file has an incorrect format or invalid values, the `apiserver` will reject the request.

The API server is the only component allowed to communicate directly with `etcd` (the core database of Kubernetes).

When a request passes all validation steps, the API server will update the **Desired State** in `etcd`.

**Pod Creation Process:**

1.  `kubectl create -f pod.yaml`
2.  `kubectl` converts the YAML file into an HTTP REST request and sends it to the `kube-apiserver`.
3.  The API server receives the request and performs Authentication, Authorization, and Validation.
4.  After all checks pass, the API server records the pod's definition in `etcd`.
5.  However, at this point, **the pod is not yet created**. The API server's job is simply to **record the desired state**.



### Kubernetes Controllers

Controllers work to ensure the cluster's current state matches the desired state. They are responsible for managing resources like Pods and Services.

*   **Reconciliation Loop:** All controllers follow this pattern:
    *   **Observe:** They continuously monitor the resource state in `etcd` through the `kube-apiserver`.
    *   **Compare:** They compare the **Desired State** (from the YAML file) with the **Current State** (the actual state in the cluster).
    *   **Act:** If the desired state is different from the current state, the controller takes action to reconcile the difference.

*   **Deployment** --> manages **ReplicaSets**
*   **ReplicaSet** --> manages the number of **Pods**
*   **Node Controller** --> runs on the control plane and is responsible for monitoring the health of worker nodes.

**Example of a ReplicaSet Controller:**

The ReplicaSet Controller constantly asks the API server: "Are there any ReplicaSet resources defined in `etcd`? How many pods are currently running for each ReplicaSet?"

*   **Scenario A - New ReplicaSet creation:** The ReplicaSet Controller discovers a new ReplicaSet in `etcd` with a desired state of 3 replicas. It then asks the API server for the current state and finds that there are 0 pods running for this ReplicaSet.
*   **Desired state (3) != Current state (0)** --> The controller takes action. It sends a request through the API server to create 3 pods.



### Services

**What is a service?** Pods can be killed, die, or be restarted. When this happens, their IP addresses may change. They are like temporary phone numbers. If the IP changes every time, how can other services find them?

This is the problem that **Services** solve.

A Service is like a permanent mailing address. When you send a request to a Service's fixed address, it acts as a load balancer and routes the traffic to the correct Pods.

**The relationship with Deployment controllers:**

When a Deployment controller updates an application, it manages the rollout of new pods. During this process, the Service ensures that traffic is only sent to healthy, updated Pods, enabling zero-downtime updates.



### Storage

By default, storage inside a Pod is **ephemeral**. If a pod dies, its storage and all the data within it are lost. If you are running a database in a Pod, the data will be gone.

**Storage Objects (PersistentVolume, PersistentVolumeClaim)** are used to fix this issue. They provide external storage that persists independently of the Pods. A Pod "claims" this storage, and even if the Pod dies, the storage volume still exists for the next pod to use.

When we define a database deployment, we also need to define a **PersistentVolumeClaim**. This tells Kubernetes that our pods require a certain amount of storage (e.g., 10GB) to store data persistently.



### Node

A Node is like a server. All pods need to run on nodes, and many nodes together form a cluster.

**Worker Node Components**

1.  **Kubelet**
    It is the primary agent for every node and represents it to the `kube-apiserver`. It receives requests and continuously communicates with the `kube-apiserver`, asking questions like, "Are there any new pods for my node?" It also reports the status of its pods and the node itself back to the `kube-apiserver`. If a pod dies, it reports this immediately.

2.  **Container Runtime**
    This is the application responsible for running containers (e.g., Docker, containerd, CRI-O). The Kubelet does not start a container directly; instead, it sends requests to the container runtime, such as "Please download the `nginx:latest` image and run it."

3.  **Kube-proxy**
    Kube-proxy is responsible for managing all network rules on the node. For example, it helps modify `iptables` rules to ensure that requests can be sent to the correct pod.

In a Kubernetes Cluster, there are two types of nodes:

1.  **Control Plane Node(s) (Master Node)**
2.  **Worker Node(s):** These follow requests from the Control Plane to run pods.



### A Full Story:

1.  As a user, I type a command to create a pod: `kubectl create pod nginx`.
2.  This command is sent to the **Control Plane Node** and received by the `kube-apiserver`.
3.  The API server validates the request and writes the new pod's definition to `etcd`.
4.  On the same control plane, the **Scheduler** discovers that there is a new pod that hasn't been assigned to a node.
5.  The Scheduler checks all the worker nodes and calculates their available resources. It decides that `worker-node-2` is free, then updates `etcd` to bind the pod to `worker-node-2`.
6.  The **Kubelet**, which is the agent for `worker-node-2`, is constantly monitoring the API server. It discovers that a new job (the nginx pod) has been assigned to it.
7.  The Kubelet reads the pod's requirements and tells the **container runtime** (e.g., Docker) on `worker-node-2` to "Please run the nginx container."
8.  The container runtime runs the container.
9.  The Kubelet checks that the container is running and then reports the pod's status back to the `kube-apiserver` on the control plane.



